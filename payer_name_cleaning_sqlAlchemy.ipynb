{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b3a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### LIMITED SUBSET OF CPT CODES ###############\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine, text, bindparam\n",
    "import pandas as pd\n",
    "\n",
    "DB = \"postgresql+psycopg2://postgres:verdansk2020!@iamr007.ddns.net:2345/hospital_db\"\n",
    "engine = create_engine(DB, pool_pre_ping=True)\n",
    "\n",
    "cpt_list = ['20610','20611','27477','27130','20600','20605','29827','29881','26055','29826']\n",
    "cpt_list = [c.strip() for c in cpt_list]\n",
    "\n",
    "sql = text(\"\"\"\n",
    "SELECT \n",
    "    hcc.description,\n",
    "    hcc.code,\n",
    "    hcc.code_type,\n",
    "    hcc.payer_name\n",
    "FROM public.hospital_cpt_charges hcc\n",
    "WHERE hcc.code IN :cpts\n",
    "\"\"\").bindparams(bindparam(\"cpts\", expanding=True))\n",
    "\n",
    "df = pd.read_sql_query(sql, engine, params={\"cpts\": cpt_list})\n",
    "df.to_csv(\"cpt_subset.csv\", index=False)\n",
    "############# SUBSET TO CSV STEP DONE #############\n",
    "\n",
    "# be sure to check above code for correct file names, password , etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393e4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ALL CSV CODES CLEANING ##########\n",
    "\n",
    "# pip install sqlalchemy psycopg2-binary pandas\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Connect to the database ---\n",
    "DB = \"postgresql+psycopg2://postgres:verdansk2020!@iamr007.ddns.net:2345/hospital_db\"\n",
    "engine = create_engine(DB, pool_pre_ping=True)\n",
    "\n",
    "# --- 2. Define the query (no WHERE filter, all CPT codes) ---\n",
    "sql = text(\"\"\"\n",
    "SELECT\n",
    "    description,\n",
    "    code,\n",
    "    code_type,\n",
    "    payer_name\n",
    "FROM public.hospital_cpt_charges\n",
    "\"\"\")\n",
    "\n",
    "# --- 3. Define output path (Windows Desktop) ---\n",
    "desktop = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "out_path = os.path.join(desktop, \"cpt_all.csv\")\n",
    "\n",
    "# --- 4. Export in chunks (handles large tables safely) ---\n",
    "chunks = pd.read_sql_query(sql, engine, chunksize=50000)\n",
    "\n",
    "first = True\n",
    "for chunk in chunks:\n",
    "    chunk.to_csv(out_path, index=False, mode=\"w\" if first else \"a\", header=first)\n",
    "    first = False\n",
    "\n",
    "print(f\"✅ Export complete. File saved to:\\n{out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c377234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to: C:\\Users\\jeffr\\OneDrive\\Desktop\\cpt_all.csv.gz\n",
      "✅ Done. File saved to:\n",
      "C:\\Users\\jeffr\\OneDrive\\Desktop\\cpt_all.csv.gz\n"
     ]
    }
   ],
   "source": [
    "############# ALTERNATE METHOD #############\n",
    "\n",
    "# pip install psycopg2-binary\n",
    "import os, gzip, psycopg2\n",
    "\n",
    "DSN = (\n",
    "    \"postgresql://postgres:verdansk2020!@iamr007.ddns.net:2345/hospital_db\"\n",
    "    \"?sslmode=disable\"\n",
    "    \"&keepalives=1&keepalives_idle=30&keepalives_interval=10&keepalives_count=10\"\n",
    ")\n",
    "\n",
    "# --- 1. Find a real Desktop path ---\n",
    "possible_desktops = [\n",
    "    os.path.join(os.path.expanduser(\"~\"), \"Desktop\"),\n",
    "    os.path.join(os.path.expanduser(\"~\"), \"OneDrive\", \"Desktop\"),\n",
    "]\n",
    "desktop = next((p for p in possible_desktops if os.path.isdir(p)), os.getcwd())\n",
    "\n",
    "# --- 2. Make sure it exists ---\n",
    "os.makedirs(desktop, exist_ok=True)\n",
    "\n",
    "out_path = os.path.join(desktop, \"cpt_all.csv.gz\")\n",
    "print(f\"Saving to: {out_path}\")\n",
    "\n",
    "# --- 3. Run COPY TO STDOUT and stream directly into a gzip file ---\n",
    "copy_sql = \"\"\"\n",
    "COPY (\n",
    "  SELECT description, code, code_type, payer_name\n",
    "  FROM public.hospital_cpt_charges\n",
    ") TO STDOUT WITH (FORMAT CSV, HEADER, QUOTE '\"', ESCAPE '\"')\n",
    "\"\"\"\n",
    "\n",
    "with psycopg2.connect(DSN) as conn:\n",
    "    with conn.cursor() as cur, gzip.open(out_path, \"wb\", compresslevel=6) as f:\n",
    "        cur.copy_expert(copy_sql, f)\n",
    "\n",
    "print(f\"✅ Done. File saved to:\\n{out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ce6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now for the cleaning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e7350cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################## FUNCTION DEFINITION ##########################\n",
    "\n",
    "# pip install pandas rapidfuzz\n",
    "import pandas as pd\n",
    "\n",
    "# -------------- your function (as given) --------------\n",
    "# paste your standardize_payers() here, unchanged (the actual function code)\n",
    "# ------------------------------------------------------\n",
    "# pip install rapidfuzz\n",
    "from typing import Iterable, Optional, Dict, Tuple, Union\n",
    "import re\n",
    "\n",
    "def standardize_payers(\n",
    "    data: Union[str, pd.DataFrame],\n",
    "    col: str = \"Payer_name\",\n",
    "    canonical: Optional[Iterable[str]] = None,\n",
    "    rules: Optional[Dict[str, str]] = None,\n",
    "    threshold: int = 90,\n",
    "    out_path: Optional[str] = None,\n",
    "    add_cols: Tuple[str, str, str] = (\"payer_clean\", \"payer_match\", \"match_score\"),\n",
    "    replace_col: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    End-to-end payer standardization:\n",
    "      - Accepts a CSV path or a DataFrame\n",
    "      - Normalizes payer strings (case, punctuation, stopwords)\n",
    "      - Applies optional explicit rules first\n",
    "      - Fuzzy-matches to a canonical list (RapidFuzz token-set ratio)\n",
    "      - Returns DataFrame, optionally writes CSV\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : str | DataFrame\n",
    "        Path to CSV or a pandas DataFrame.\n",
    "    col : str\n",
    "        Column with payer names to clean.\n",
    "    canonical : Iterable[str] | None\n",
    "        Canonical payer display names. If None, derives top values from data.\n",
    "    rules : dict | None\n",
    "        Explicit mappings applied BEFORE fuzzy match. Raw strings; normalization handled inside.\n",
    "        Example: {\"uhc\": \"UnitedHealthcare\", \"blue cross blueshield\": \"Blue Cross Blue Shield\"}\n",
    "    threshold : int\n",
    "        Minimum RapidFuzz score (0-100) to accept a match. Below => match is None.\n",
    "    out_path : str | None\n",
    "        If provided, writes the resulting DataFrame to this CSV path.\n",
    "    add_cols : (str, str, str)\n",
    "        Names for (normalized_text, canonical_match, score) columns to add.\n",
    "    replace_col : bool\n",
    "        If True, replaces `col` with the canonical match (when available).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Original plus added columns; optionally with `col` replaced.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from rapidfuzz import process, fuzz\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Install rapidfuzz: pip install rapidfuzz\") from e\n",
    "\n",
    "    # --- Load ---\n",
    "    if isinstance(data, str):\n",
    "        df = pd.read_csv(data)\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        df = data.copy()\n",
    "    else:\n",
    "        raise TypeError(\"`data` must be a CSV path or a pandas DataFrame\")\n",
    "\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "\n",
    "    # --- Helpers (scoped inside so this is truly a single-function drop-in) ---\n",
    "    _STOPWORDS = {\n",
    "        \"inc\", \"inc.\", \"llc\", \"l.l.c.\", \"corp\", \"co\", \"co.\", \"company\",\n",
    "        \"insurance\", \"ins\", \"health\", \"healthcare\", \"plan\", \"plans\", \"the\",\n",
    "        \"of\", \"services\"\n",
    "    }\n",
    "\n",
    "    def _normalize(s: str) -> str:\n",
    "        if s is None:\n",
    "            return \"\"\n",
    "        s = str(s).lower()\n",
    "        s = s.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "        s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        tokens = [t for t in s.split() if t not in _STOPWORDS]\n",
    "        tokens = sorted(set(tokens))  # token-set behavior\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def _prep_canonical(canon_iter: Iterable[str]) -> Dict[str, str]:\n",
    "        # map normalized -> display\n",
    "        return {_normalize(name): name for name in canon_iter}\n",
    "\n",
    "    def _prep_rules(rules_dict: Dict[str, str]) -> Dict[str, str]:\n",
    "        # normalize keys only; values are display strings\n",
    "        return {_normalize(k): v for k, v in rules_dict.items()}\n",
    "\n",
    "    # --- Normalize incoming values ---\n",
    "    payer_clean_col, payer_match_col, score_col = add_cols\n",
    "    df[payer_clean_col] = df[col].map(_normalize)\n",
    "\n",
    "    # --- Canonical list ---\n",
    "    if canonical is None:\n",
    "        # derive from the data’s most frequent normalized forms\n",
    "        top_norm = (\n",
    "            df[payer_clean_col].value_counts(dropna=True).head(200).index.tolist()\n",
    "        )\n",
    "        canonical_display = [t.title() for t in top_norm if t]\n",
    "    else:\n",
    "        canonical_display = list(canonical)\n",
    "\n",
    "    canon_map = _prep_canonical(canonical_display)       # norm -> display\n",
    "    canon_norms = list(canon_map.keys())\n",
    "    rule_map = _prep_rules(rules) if rules else {}\n",
    "\n",
    "    # --- Fuzzy match each normalized value ---\n",
    "    def _match_one(norm_val: str):\n",
    "        # Rules first (exact on normalized form)\n",
    "        if norm_val in rule_map:\n",
    "            return rule_map[norm_val], 100\n",
    "\n",
    "        if not norm_val:\n",
    "            return None, 0\n",
    "\n",
    "        best = process.extractOne(\n",
    "            norm_val, canon_norms, scorer=fuzz.token_set_ratio, score_cutoff=threshold\n",
    "        )\n",
    "        if best is None:\n",
    "            return None, 0\n",
    "        best_norm, score, _ = best\n",
    "        return canon_map[best_norm], int(score)\n",
    "\n",
    "    matches = df[payer_clean_col].apply(_match_one)\n",
    "    df[payer_match_col] = matches.map(lambda x: x[0])\n",
    "    df[score_col] = matches.map(lambda x: x[1])\n",
    "\n",
    "    # Optionally replace original column with the canonical match when present\n",
    "    if replace_col:\n",
    "        df[col] = df[payer_match_col].where(df[payer_match_col].notna(), df[col])\n",
    "\n",
    "    # Optional write-out\n",
    "    if out_path:\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52fd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffr\\AppData\\Local\\Temp\\ipykernel_53124\\663097329.py:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_subset = pd.read_csv(r\"C:\\Users\\jeffr\\OneDrive\\Desktop\\cpt_all.csv\", usecols=[\"code\",\"payer_name\"])  # adjust names if needed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read only the two columns you need (rename if your headers differ)\n",
    "df_subset = pd.read_csv(r\"C:\\Users\\jeffr\\OneDrive\\Desktop\\cpt_all.csv\", usecols=[\"code\",\"payer_name\"])  # adjust names if needed\n",
    "\n",
    "# Optional: a starter canonical list + rules (edit these)\n",
    "canonical = [\n",
    "    \"Medicare\", \"Florida Medicaid\", \"UnitedHealthcare\",\n",
    "    \"Blue Cross Blue Shield of Florida\", \"Aetna\", \"Cigna\",\n",
    "    \"Humana\", \"TRICARE\", \"Molina Healthcare\", \"Ambetter (Centene)\",\n",
    "    \"Sunshine Health (Centene)\", \"AvMed\", \"Oscar\", \"GEHA\"\n",
    "]\n",
    "\n",
    "# REMINDER: \n",
    "# .lower on initial cells\n",
    "# strip spaces in the cells\n",
    "# for loop to search plan name in all lower and stripped (ex. unitedhealthcare)\n",
    "# KEEP PLAN NAME A VARIABLE SO FOR LOOP CAN BE REUSED\n",
    "# group into buckets\n",
    "\n",
    "rules = {\n",
    "    \"uhc\": \"UnitedHealthcare\",\n",
    "    \"united health care\": \"UnitedHealthcare\",\n",
    "    \"florida blue\": \"Blue Cross Blue Shield of Florida\",\n",
    "    \"bcbs\": \"Blue Cross Blue Shield of Florida\",\n",
    "    \"blue cross blueshield\": \"Blue Cross Blue Shield of Florida\",\n",
    "    \"aetna inc\": \"Aetna\",\n",
    "    \"cvs aetna\": \"Aetna\",\n",
    "    \"cigna healthcare\": \"Cigna\",\n",
    "    \"tricare east\": \"TRICARE\",\n",
    "    \"usaa tricare\": \"TRICARE\",\n",
    "    \"cms medicare\": \"Medicare\",\n",
    "    \"medicare part b\": \"Medicare\",\n",
    "    \"medicaid fl\": \"Florida Medicaid\",\n",
    "    \"sunshine\": \"Sunshine Health (Centene)\",\n",
    "    \"ambetter\": \"Ambetter (Centene)\",\n",
    "    \"molina\": \"Molina Healthcare\",\n",
    "    \"avmed health\": \"AvMed\"\n",
    "}\n",
    "\n",
    "# Run the cleaner; replace the payer column with the canonical name\n",
    "df_clean = standardize_payers(\n",
    "    data=df_subset,\n",
    "    col=\"payer_name\",\n",
    "    canonical=canonical,     # or None to learn from data\n",
    "    rules=rules,             # optional\n",
    "    threshold=90,\n",
    "    #out_path = r\"C:\\Users\\<YOUR_USERNAME>\\Desktop\\cpt_subset_cleaned_rows.csv\"   OPTIONAL\n",
    "    add_cols=(\"payer_norm\",\"payer_canonical\",\"match_score\"),\n",
    "    replace_col=True\n",
    ")\n",
    "print(df_clean.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e238a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# ALTERNATE METHOD #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6a56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIG ---\n",
    "IN_CSV  = r\"C:\\Users\\jeffr\\OneDrive\\Desktop\\cpt_all.csv\"\n",
    "OUT_CSV = r\"C:\\Users\\jeffr\\Desktop\\cpt_all_cleaned.csv\"\n",
    "PAYER_COL = \"payer_name\"          # <-- set to your real column name\n",
    "CHUNKSIZE = 250_000               # tune for your machine\n",
    "\n",
    "# Canonical & rules (same idea as before; edit to taste)\n",
    "CANONICAL = [\n",
    "    \"Medicare\", \"Florida Medicaid\", \"UnitedHealthcare\",\n",
    "    \"Blue Cross Blue Shield of Florida\", \"Aetna\", \"Cigna\",\n",
    "    \"Humana\", \"TRICARE\", \"Molina Healthcare\", \"Ambetter (Centene)\",\n",
    "    \"Sunshine Health (Centene)\", \"AvMed\", \"Oscar\", \"GEHA\"\n",
    "]\n",
    "RULES = {\n",
    "    \"uhc\": \"UnitedHealthcare\",\n",
    "    \"united health care\": \"UnitedHealthcare\",\n",
    "    \"florida blue\": \"Blue Cross Blue Shield of Florida\",\n",
    "    \"bcbs\": \"Blue Cross Blue Shield of Florida\",\n",
    "    \"blue cross blueshield\": \"Blue Cross Blue Shield of Florida\",\n",
    "    \"aetna inc\": \"Aetna\",\n",
    "    \"cvs aetna\": \"Aetna\",\n",
    "    \"cigna healthcare\": \"Cigna\",\n",
    "    \"tricare east\": \"TRICARE\",\n",
    "    \"usaa tricare\": \"TRICARE\",\n",
    "    \"cms medicare\": \"Medicare\",\n",
    "    \"medicare part b\": \"Medicare\",\n",
    "    \"medicaid fl\": \"Florida Medicaid\",\n",
    "    \"sunshine\": \"Sunshine Health (Centene)\",\n",
    "    \"ambetter\": \"Ambetter (Centene)\",\n",
    "    \"molina\": \"Molina Healthcare\",\n",
    "    \"avmed health\": \"AvMed\"\n",
    "}\n",
    "THRESHOLD = 90\n",
    "\n",
    "# --- Normalizer (same as your function) ---\n",
    "_STOP = {\"inc\",\"inc.\",\"llc\",\"l.l.c.\",\"corp\",\"co\",\"co.\",\"company\",\"insurance\",\"ins\",\n",
    "         \"health\",\"healthcare\",\"plan\",\"plans\",\"the\",\"of\",\"services\"}\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = str(s).lower()\n",
    "    s = s.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    toks = [t for t in s.split() if t not in _STOP]\n",
    "    toks = sorted(set(toks))\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# Prep canonical\n",
    "canon_map = {_normalize(x): x for x in CANONICAL}\n",
    "canon_norms = list(canon_map.keys())\n",
    "rule_map = {_normalize(k): v for k, v in RULES.items()}\n",
    "\n",
    "# ---------- PASS 1: collect unique payer strings ----------\n",
    "uniq = set()\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    IN_CSV,\n",
    "    usecols=[PAYER_COL],\n",
    "    dtype={PAYER_COL: \"string\"},\n",
    "    chunksize=CHUNKSIZE,\n",
    "    low_memory=False\n",
    "):\n",
    "    s = chunk[PAYER_COL].dropna().astype(\"string\")\n",
    "    uniq.update(s.unique().tolist())\n",
    "\n",
    "# ---------- Build mapping (raw -> (canonical, score, norm)) ----------\n",
    "def match_one(raw: str):\n",
    "    norm = _normalize(raw)\n",
    "    if norm in rule_map:\n",
    "        return (rule_map[norm], 100, norm)\n",
    "    if not norm:\n",
    "        return (None, 0, norm)\n",
    "    best = process.extractOne(norm, canon_norms, scorer=fuzz.token_set_ratio, score_cutoff=THRESHOLD)\n",
    "    if best is None:\n",
    "        return (None, 0, norm)\n",
    "    best_norm, score, _ = best\n",
    "    return (canon_map[best_norm], int(score), norm)\n",
    "\n",
    "# Precompute the mapping only for UNIQUE values\n",
    "mapping = {}\n",
    "for val in uniq:\n",
    "    mapping[val] = match_one(val)\n",
    "\n",
    "# ---------- PASS 2: stream, apply mapping, write out ----------\n",
    "# Ensure output dir exists; remove old file if re-running\n",
    "Path(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "first = True\n",
    "\n",
    "usecols = None  # None = keep all columns; or list specific columns if you want\n",
    "dtype_hint = {PAYER_COL: \"string\"}  # force payer column to string\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    IN_CSV,\n",
    "    usecols=usecols,\n",
    "    dtype=dtype_hint,\n",
    "    chunksize=CHUNKSIZE,\n",
    "    low_memory=False\n",
    "):\n",
    "    # Lookup using raw string (exact, fast). For unmapped, leave original.\n",
    "    raw = chunk[PAYER_COL].astype(\"string\")\n",
    "    # Vectorized map to tuple; then split out to columns\n",
    "    tup = raw.map(mapping).fillna((None, 0, \"\"))\n",
    "\n",
    "    chunk[\"payer_norm\"]       = tup.map(lambda x: x[2])\n",
    "    chunk[\"payer_canonical\"]  = tup.map(lambda x: x[0])\n",
    "    chunk[\"match_score\"]      = tup.map(lambda x: x[1])\n",
    "\n",
    "    # If you want to REPLACE the original payer with canonical when available:\n",
    "    chunk[PAYER_COL] = chunk[\"payer_canonical\"].where(chunk[\"payer_canonical\"].notna(), raw)\n",
    "\n",
    "    # Write/append\n",
    "    chunk.to_csv(OUT_CSV, index=False, mode=\"w\" if first else \"a\", header=first, encoding=\"utf-8-sig\")\n",
    "    first = False\n",
    "\n",
    "print(\"Done. Wrote:\", OUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00741f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF I WANT TO FINALIZE AND SAVE THE FILE\n",
    "\n",
    "\n",
    "df_clean.to_csv(r\"C:\\Users\\<YOUR_USERNAME>\\Desktop\\cleaned_payers.csv\", index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
