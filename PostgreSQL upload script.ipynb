{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8005a560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\gio12\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\gio12\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\gio12\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gio12\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gio12\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gio12\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading psycopg2_binary-2.9.10-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 29.1 MB/s eta 0:00:00\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, psycopg2-binary\n",
      "\n",
      "   -------------------- ------------------- 1/2 [psycopg2-binary]\n",
      "   ---------------------------------------- 2/2 [psycopg2-binary]\n",
      "\n",
      "Successfully installed psycopg2-binary-2.9.10 python-dotenv-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Bulk‑load a folder of CSV files (all same schema) into a PostgreSQL table.\n",
    "\n",
    "Features\n",
    "--------\n",
    "- Fast COPY FROM STDIN via psycopg2\n",
    "- Optional auto‑create table (schema inferred from first CSV)\n",
    "- Optional truncate before load\n",
    "- Simple logging + dry‑run mode\n",
    "\n",
    "Requirements\n",
    "------------\n",
    "pip install psycopg2-binary pandas python-dotenv\n",
    "(or use psycopg2 if you have libpq installed)\n",
    "\n",
    "Examples\n",
    "--------\n",
    "python load_csv_to_postgres.py \\\n",
    "    --folder /path/to/csvs \\\n",
    "    --table public.my_table \\\n",
    "    --host localhost --port 5432 --user myuser --password mypass --db mydb \\\n",
    "    --delimiter \",\" --encoding \"utf-8\" --truncate --autocreate\n",
    "\n",
    "You can also use environment variables instead of flags:\n",
    "  PGHOST, PGPORT, PGUSER, PGPASSWORD, PGDATABASE\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# --------------- Helpers ---------------\n",
    "\n",
    "def infer_pg_type(pd_dtype: str) -> str:\n",
    "    \"\"\"Map pandas dtype string to a reasonable PostgreSQL type.\"\"\"\n",
    "    # You can tweak this mapping as needed\n",
    "    if pd_dtype.startswith(\"int\"):\n",
    "        return \"BIGINT\"\n",
    "    if pd_dtype.startswith(\"float\"):\n",
    "        return \"DOUBLE PRECISION\"\n",
    "    if pd_dtype == \"bool\":\n",
    "        return \"BOOLEAN\"\n",
    "    if \"datetime\" in pd_dtype:\n",
    "        return \"TIMESTAMP\"\n",
    "    if \"date\" in pd_dtype:\n",
    "        return \"DATE\"\n",
    "    return \"TEXT\"  # default/fallback\n",
    "\n",
    "def infer_schema_from_csv(csv_path: Path, delimiter: str, encoding: str) -> List[str]:\n",
    "    \"\"\"Return a list of column defs like ['col1 TEXT', 'col2 BIGINT', ...] inferred from the first CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path, nrows=5000, dtype=str, sep=delimiter, encoding=encoding, keep_default_na=True)\n",
    "    # Try to infer numeric/bool/datetime\n",
    "    sample = pd.read_csv(csv_path, nrows=5000, sep=delimiter, encoding=encoding, keep_default_na=True)\n",
    "    coldefs = []\n",
    "    for col in df.columns:\n",
    "        dtype = str(sample[col].infer_objects().dtype)\n",
    "        # pandas sometimes keeps ints as floats, try to refine\n",
    "        try:\n",
    "            # If values are integers when parsed as float with no fractional part -> treat as BIGINT\n",
    "            s = pd.to_numeric(sample[col], errors=\"coerce\")\n",
    "            if pd.api.types.is_integer_dtype(s.dropna()):\n",
    "                dtype = \"int64\"\n",
    "            elif pd.api.types.is_float_dtype(s.dropna()):\n",
    "                dtype = \"float64\"\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Try datetimes\n",
    "        if dtype == \"object\":\n",
    "            try:\n",
    "                pd.to_datetime(sample[col].dropna().head(100), errors=\"raise\", infer_datetime_format=True)\n",
    "                dtype = \"datetime64[ns]\"\n",
    "            except Exception:\n",
    "                pass\n",
    "        coldefs.append(f'{psql_ident(col)} {infer_pg_type(dtype)}')\n",
    "    return coldefs\n",
    "\n",
    "def psql_ident(identifier: str) -> str:\n",
    "    \"\"\"Safely quote an identifier (lowercase it for consistency).\"\"\"\n",
    "    # We will lowercase identifiers to avoid case-sensitivity headaches.\n",
    "    # Surround with double quotes if special chars are present.\n",
    "    safe = identifier.strip().lower().replace('\"', '\"\"')\n",
    "    return f'\"{safe}\"'\n",
    "\n",
    "def table_exists(cur, schema: str, table: str) -> bool:\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT 1\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema = %s AND table_name = %s\n",
    "        \"\"\",\n",
    "        (schema, table)\n",
    "    )\n",
    "    return cur.fetchone() is not None\n",
    "\n",
    "def create_table(cur, full_table: str, columns: List[str]):\n",
    "    ddl = f\"CREATE TABLE IF NOT EXISTS {full_table} (\\n  {', '.join(columns)}\\n);\"\n",
    "    cur.execute(ddl)\n",
    "\n",
    "def truncate_table(cur, full_table: str):\n",
    "    cur.execute(f\"TRUNCATE TABLE {full_table};\")\n",
    "\n",
    "def copy_csv(cur, full_table: str, csv_path: Path, delimiter: str, encoding: str):\n",
    "    with open(csv_path, \"r\", encoding=encoding, newline=\"\") as f:\n",
    "        # Use COPY with explicit column list (based on header) to ensure correct mapping\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "        header = next(reader)\n",
    "        columns = \", \".join(psql_ident(c) for c in header)\n",
    "        f.seek(0)\n",
    "        copy_sql = f\"\"\"\n",
    "            COPY {full_table} ({columns})\n",
    "            FROM STDIN WITH (FORMAT csv, HEADER true, DELIMITER '{delimiter}');\n",
    "        \"\"\"\n",
    "        cur.copy_expert(copy_sql, f)\n",
    "\n",
    "# --------------- Main ---------------\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Bulk‑load a folder of CSVs into PostgreSQL.\")\n",
    "    ap.add_argument(\"--folder\", required=True, help=\"Folder containing CSV files (all same columns/order)\")\n",
    "    ap.add_argument(\"--table\", required=True, help=\"Target table in the form schema.table (e.g., public.my_table)\")\n",
    "    ap.add_argument(\"--host\", default=os.getenv(\"PGHOST\", \"localhost\"))\n",
    "    ap.add_argument(\"--port\", type=int, default=int(os.getenv(\"PGPORT\", \"5432\")))\n",
    "    ap.add_argument(\"--user\", default=os.getenv(\"PGUSER\"))\n",
    "    ap.add_argument(\"--password\", default=os.getenv(\"PGPASSWORD\"))\n",
    "    ap.add_argument(\"--db\", default=os.getenv(\"PGDATABASE\"))\n",
    "    ap.add_argument(\"--delimiter\", default=\",\")\n",
    "    ap.add_argument(\"--encoding\", default=\"utf-8\")\n",
    "    ap.add_argument(\"--truncate\", action=\"store_true\", help=\"Truncate the table before loading\")\n",
    "    ap.add_argument(\"--autocreate\", action=\"store_true\", help=\"Create the table (schema inferred from first CSV) if it does not exist\")\n",
    "    ap.add_argument(\"--dry_run\", action=\"store_true\", help=\"Show what would happen without writing any data\")\n",
    "\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    folder = Path(args.folder).expanduser().resolve()\n",
    "    if not folder.is_dir():\n",
    "        print(f\"[ERROR] Folder not found: {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if \".\" not in args.table:\n",
    "        print(\"[ERROR] --table must be schema.table (e.g., public.my_table)\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    schema, table = args.table.split(\".\", 1)\n",
    "    full_table = f'{psql_ident(schema)}.{psql_ident(table)}'\n",
    "\n",
    "    csv_files = sorted([p for p in folder.glob(\"*.csv\") if p.is_file()])\n",
    "    if not csv_files:\n",
    "        print(f\"[ERROR] No CSV files found in {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Connect\n",
    "    conn = psycopg2.connect(\n",
    "        host=args.host,\n",
    "        port=args.port,\n",
    "        user=args.user,\n",
    "        password=args.password,\n",
    "        dbname=args.db,\n",
    "    )\n",
    "    conn.autocommit = False\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create table if requested\n",
    "            exists = table_exists(cur, schema, table)\n",
    "            if not exists and args.autocreate:\n",
    "                print(f\"[INFO] Inferring schema from: {csv_files[0].name}\")\n",
    "                coldefs = infer_schema_from_csv(csv_files[0], args.delimiter, args.encoding)\n",
    "                print(f\"[INFO] Creating table {args.table} with columns:\")\n",
    "                for c in coldefs:\n",
    "                    print(f\"       {c}\")\n",
    "                if not args.dry_run:\n",
    "                    create_table(cur, full_table, coldefs)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    print(\"[DRY‑RUN] Skipping CREATE TABLE\")\n",
    "\n",
    "            elif not exists and not args.autocreate:\n",
    "                print(f\"[ERROR] Target table {args.table} does not exist. Use --autocreate to create it.\", file=sys.stderr)\n",
    "                sys.exit(1)\n",
    "\n",
    "            # Optionally truncate\n",
    "            if args.truncate:\n",
    "                print(f\"[INFO] Truncating {args.table}\")\n",
    "                if not args.dry_run:\n",
    "                    truncate_table(cur, full_table)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    print(\"[DRY‑RUN] Skipping TRUNCATE\")\n",
    "\n",
    "            # Load files\n",
    "            total_rows = 0\n",
    "            for csv_path in csv_files:\n",
    "                print(f\"[INFO] Loading {csv_path.name} ...\")\n",
    "                if not args.dry_run:\n",
    "                    try:\n",
    "                        copy_csv(cur, full_table, csv_path, args.delimiter, args.encoding)\n",
    "                        conn.commit()\n",
    "                    except Exception as e:\n",
    "                        conn.rollback()\n",
    "                        print(f\"[ERROR] Failed to load {csv_path.name}: {e}\", file=sys.stderr)\n",
    "                        sys.exit(1)\n",
    "                # We don't have an exact row count without parsing; optionally estimate:\n",
    "                try:\n",
    "                    # Count lines minus header efficiently\n",
    "                    with open(csv_path, \"r\", encoding=args.encoding, newline=\"\") as f:\n",
    "                        row_count = sum(1 for _ in f) - 1\n",
    "                        total_rows += max(0, row_count)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            print(f\"[DONE] Loaded {len(csv_files)} file(s). Approx rows: {total_rows}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
